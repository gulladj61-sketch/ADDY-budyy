<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/>
<title>ADDY Backend (iframe)</title>
<style>
  :root{--bg:#08121a;--muted:#9fd8ea;--accent:#00e6ff}
  body{margin:0;background:linear-gradient(180deg,#06121a,#051825);color:#eaffff;font-family:Inter,system-ui,'Poppins',sans-serif;padding:14px}
  h2{color:var(--accent)}
  .log{font-size:13px;color:var(--muted);opacity:0.9}
  pre{background:rgba(255,255,255,0.02);padding:8px;border-radius:8px;overflow:auto}
  .small{font-size:13px;color:var(--muted)}
  .card{background:linear-gradient(180deg,rgba(255,255,255,0.01),transparent);padding:10px;border-radius:8px;border:1px solid rgba(255,255,255,0.02)}
  .row{display:flex;gap:8px;align-items:center}
  input[type=text]{padding:8px;border-radius:8px;border:1px solid rgba(255,255,255,0.03);background:transparent;color:#eaffff;width:100%}
  button{background:var(--accent);color:#012;padding:8px 10px;border-radius:8px;border:0;cursor:pointer}
</style>
</head>
<body>
  <h2>ADDY Backend — iframe</h2>
  <div class="small">This frame receives messages from parent and processes according to selected tool. Place API keys in Settings (below) to enable real models.</div>

  <div style="margin-top:12px" class="card">
    <div class="small">Status: <span id="status">ready</span></div>
    <div style="margin-top:8px" class="row">
      <input id="hfKey" type="text" placeholder="HuggingFace token (optional)">
      <input id="openaiKey" type="text" placeholder="OpenAI / Gemini token (optional)">
      <button id="saveBtn">Save</button>
    </div>
    <div style="margin-top:8px" class="small">Keys stored locally in browser storage (for demo only). For production - use server proxy.</div>
  </div>

  <div style="margin-top:12px">
    <h3 class="small">Log / Last activity</h3>
    <pre id="log" style="height:220px">Waiting for messages from parent...</pre>
  </div>

<!-- include Tesseract for OCR -->
<script src="https://cdn.jsdelivr.net/npm/tesseract.js@4.1.1/dist/tesseract.min.js"></script>

<script>
/* ---------- simple backend message handler ---------- */
const logBox = document.getElementById('log'), statusEl = document.getElementById('status');
const hfInput = document.getElementById('hfKey'), openaiInput = document.getElementById('openaiKey'), saveBtn = document.getElementById('saveBtn');

function log(msg){ const t = new Date().toLocaleTimeString(); logBox.textContent = `[${t}] ${msg}\n` + logBox.textContent; parent.postMessage({type:'log', msg}, '*'); }
function setStatus(t){ statusEl.innerText = t; parent.postMessage({type:'log', msg: 'status: '+t}, '*'); }

saveBtn.addEventListener('click', ()=>{ localStorage.setItem('hf_token', hfInput.value.trim()); localStorage.setItem('openai_key', openaiInput.value.trim()); log('API keys saved locally'); });

/* upon load, notify parent ready */
window.addEventListener('load', ()=>{ parent.postMessage({type:'ready'}, '*'); });

/* utility: send result back */
function sendResult(payload){
  // payload: { text, html, file:{name, data} }
  parent.postMessage(Object.assign({type:'result'}, payload), '*');
}

/* message listener */
window.addEventListener('message', async (ev)=>{
  const d = ev.data;
  if(!d) return;
  if(d.type === 'ping'){ log('ping received'); parent.postMessage({type:'ready'}, '*'); return; }
  if(d.type === 'toolChange'){ log('Tool selected: '+ d.tool); setStatus('tool: '+d.tool); return; }
  if(d.type === 'process'){
    const tool = d.tool || 'Chat'; const prompt = d.prompt || ''; log(`Processing tool=${tool} prompt=${prompt ? prompt.slice(0,80) : '(file)'} `);
    setStatus('processing '+tool);
    try{
      // route tools
      if(tool === 'Chat'){ await handleChat(prompt); }
      else if(tool === 'Image Enhance'){ await handleImageEnhance(d.file, prompt); }
      else if(tool === 'Image Gen'){ await handleImageGen(prompt); }
      else if(tool === 'Background Remove'){ await handleBGRemove(d.file); }
      else if(tool === 'OCR'){ await handleOCR(d.file); }
      else if(tool === 'Translator'){ await handleTranslate(prompt); }
      else if(tool === 'Voice Enhance'){ await handleVoiceEnhance(d.file); }
      else if(tool === 'Voice Transcribe'){ await handleVoiceTranscribe(d.file); }
      else if(tool === 'Text Summarize'){ await handleSummarize(prompt); }
      else if(tool === 'Text Rewrite'){ await handleRewrite(prompt); }
      else if(tool === 'TTS'){ await handleTTS(prompt); }
      else if(tool === 'Meme Maker'){ await handleMeme(d.file, prompt); }
      else if(tool === 'Code Assist'){ await handleCodeAssist(prompt); }
      else if(tool === 'Upscale'){ await handleUpscale(d.file); }
      else if(tool === 'Face Enhance'){ await placeholderTool('Face Enhance'); }
      else if(tool === 'Video Enhance'){ await placeholderTool('Video Enhance'); }
      else if(tool === 'PDF Summarize'){ await placeholderTool('PDF Summarize'); }
      else if(tool === 'Semantic Search'){ await placeholderTool('Semantic Search'); }
      else if(tool === 'Batch Processor'){ await placeholderTool('Batch Processor'); }
      else if(tool === 'Export Package'){ await placeholderTool('Export Package'); }
      else { await placeholderTool(tool); }
    }catch(err){ log('Error: '+err.message); sendResult({text:'Error: '+err.message}); }
    setStatus('idle');
  }
});

/* -------------------------
   Tool implementations (client-side / API optional)
   ------------------------- */

/* 1) Chat: simulated or OpenAI if key present */
async function handleChat(prompt){
  const key = localStorage.getItem('openai_key') || openaiInput.value.trim();
  if(key){
    // example: call OpenAI chat (user must set key in Settings) - NOTE: direct browser use is insecure
    sendResult({text:'Calling OpenAI/Gemini not allowed from demo (use server). Use local simulated reply instead.'});
    return;
  }
  // simulated reply
  const reply = simulatedChatReply(prompt);
  sendResult({text:reply});
}
function simulatedChatReply(p){
  if(!p) return "Hi — ask me anything or select a tool.";
  if(p.toLowerCase().includes('enhance')) return "Try uploading the image and use 'Image Enhance' tool. Use prompts like: 'HD, vivid, reduce blur'.";
  const samples = [
    "I can help: upload an image (Image Enhance), record audio (Voice Enhance), or paste text (Text Summarize).",
    "For translations, select 'Translator' and type the text then click send.",
    "This is a demo reply. For production connect a real model in Settings."
  ];
  return samples[Math.floor(Math.random()*samples.length)];
}

/* 2) Image Enhance: client-side pipeline (base64 file expected) */
async function handleImageEnhance(file, prompt){
  if(!file || !file.data) return sendResult({text:'No image uploaded. Use parent Upload button.'});
  // data URL -> Image
  const img = await loadImageFromDataURL(file.data);
  // create canvas
  const canvas = document.createElement('canvas'); const ctx = canvas.getContext('2d');
  // fit
  const maxW = 1600; const ratio = Math.min(1, maxW / img.width);
  canvas.width = Math.round(img.width * ratio); canvas.height = Math.round(img.height * ratio);
  ctx.drawImage(img,0,0,canvas.width,canvas.height);
  // upscale-ish: draw to bigger canvas then apply sharpen then downscale for perceived detail
  const up = document.createElement('canvas'); up.width = Math.min(canvas.width*2, 2200); up.height = Math.min(canvas.height*2, 1600);
  const uctx = up.getContext('2d'); uctx.imageSmoothingEnabled=true; uctx.imageSmoothingQuality='high';
  uctx.drawImage(canvas,0,0,up.width,up.height);
  // simple unsharp filter (small)
  try{
    let imgd = uctx.getImageData(0,0,up.width,up.height);
    imgd = unsharpSimple(imgd, up.width, up.height, prompt && prompt.toLowerCase().includes('sharp') ? 1.2 : 0.9);
    uctx.putImageData(imgd, 0,0);
  }catch(e){ console.warn(e); }
  // vibrance & contrast tweak
  vibrance(uctx, up.width, up.height, prompt && prompt.toLowerCase().includes('vivid')?1.12:1.06);
  // downscale
  const out = document.createElement('canvas'); out.width = canvas.width; out.height = canvas.height;
  const octx = out.getContext('2d'); octx.imageSmoothingEnabled=true; octx.imageSmoothingQuality='high';
  octx.drawImage(up,0,0,out.width,out.height);
  // return data URL
  const dataUrl = out.toDataURL('image/png');
  sendResult({html:'Image enhanced (client-side)', file:{name:'enhanced.png', data: dataUrl}});
}

/* utility: load dataURL into Image element */
function loadImageFromDataURL(dataUrl){
  return new Promise((res, rej)=>{
    const img = new Image();
    img.onload = ()=>res(img);
    img.onerror = e=>rej(e);
    img.src = dataUrl;
  });
}

/* simple unsharp (same technique) */
function unsharpSimple(imageData,w,h,amount=1.0){
  const data = imageData.data, copy = new Uint8ClampedArray(data);
  const k=[0,-1,0,-1,5,-1,0,-1,0];
  for(let y=1;y<h-1;y++) for(let x=1;x<w-1;x++){
    for(let c=0;c<3;c++){
      let sum=0,ki=0;
      for(let ky=-1;ky<=1;ky++) for(let kx=-1;kx<=1;kx++){ const p=(y+ky)*w+(x+kx); sum += copy[p*4 + c]*k[ki++]; }
      const idx=(y*w+x)*4+c; let val = copy[idx] + (sum - copy[idx]) * (amount*0.45);
      data[idx]=Math.max(0,Math.min(255,val));
    }
    data[(y*w+x)*4+3]=255;
  }
  return imageData;
}
function vibrance(ctx,w,h,f=1.06){
  try{
    const img = ctx.getImageData(0,0,w,h), d = img.data;
    for(let i=0;i<d.length;i+=4){
      d[i]=clamp((d[i]-128)*f+128); d[i+1]=clamp((d[i+1]-128)*f+128); d[i+2]=clamp((d[i+2]-128)*f+128);
    }
    ctx.putImageData(img,0,0);
  }catch(e){ console.warn(e); }
}
function clamp(v){ return Math.max(0,Math.min(255,Math.round(v))); }

/* 3) Image Gen: placeholder or HF inference if token present */
async function handleImageGen(prompt){
  const hf = localStorage.getItem('hf_token');
  if(!hf) return sendResult({text:'Image generation placeholder. Save HuggingFace token in Settings and use "Generate via API".'});
  sendResult({text:'Calling HuggingFace model...'});

  try{
    // example for stability model - user must ensure model supports text->image inference endpoint
    const res = await fetch('https://api-inference.huggingface.co/models/stabilityai/stable-diffusion-2', {
      method:'POST', headers:{Authorization: 'Bearer '+hf, 'Content-Type':'application/json'}, body: JSON.stringify({inputs: prompt})
    });
    if(!res.ok) throw new Error('HF returned '+res.status);
    const blob = await res.blob();
    const url = URL.createObjectURL(blob);
    sendResult({html:'Generated image (via HF)', file:{name:'gen.png', data: url}});
  }catch(e){ sendResult({text:'HF error: '+e.message}); }
}

/* 4) Background remove: naive client demo */
async function handleBGRemove(file){
  if(!file || !file.data) return sendResult({text:'Upload an image first.'});
  const img = await loadImageFromDataURL(file.data);
  const c = document.createElement('canvas'); c.width = img.width; c.height = img.height;
  const cx = c.getContext('2d'); cx.drawImage(img,0,0);
  const id = cx.getImageData(0,0,c.width,c.height);
  for(let i=0;i<id.data.length;i+=4){
    const r = id.data[i], g = id.data[i+1], b = id.data[i+2];
    if(r>220 && g>220 && b>220) id.data[i+3] = 0;
  }
  cx.putImageData(id,0,0);
  const dataUrl = c.toDataURL('image/png');
  sendResult({text:'Background removed (demo)', file:{name:'bg_removed.png', data: dataUrl}});
}

/* 5) OCR — uses Tesseract.js */
async function handleOCR(file){
  if(!file || !file.data) return sendResult({text:'Upload image first for OCR.'});
  sendResult({text:'Running OCR...'});
  try{
    const worker = Tesseract.createWorker({logger:m=>log('OCR: '+(m.status||'')+' '+(m.progress?Math.round(m.progress*100):''))});
    await worker.load(); await worker.loadLanguage('eng'); await worker.initialize('eng');
    const { data: { text } } = await worker.recognize(file.data);
    await worker.terminate();
    sendResult({text:'OCR result:\n\n'+text});
  }catch(e){ sendResult({text:'OCR failed: '+e.message}); }
}

/* 6) Translator — LibreTranslate public */
async function handleTranslate(prompt){
  if(!prompt) return sendResult({text:'Type text to translate.'});
  sendResult({text:'Translating... (LibreTranslate public)'});
  try{
    const res = await fetch('https://libretranslate.de/translate',{
      method:'POST', headers:{'Content-Type':'application/json'}, body: JSON.stringify({q:prompt, source:'auto', target:'en', format:'text'})
    });
    const j = await res.json();
    sendResult({text:'Translated:\n\n'+ (j.translatedText || JSON.stringify(j))});
  }catch(e){ sendResult({text:'Translate error: '+e.message}); }
}

/* 7) Voice Enhance — expects file.data (base64) -> returns wav blob URL */
async function handleVoiceEnhance(file){
  if(!file || !file.data) return sendResult({text:'Upload audio file first.'});
  sendResult({text:'Processing audio (offline)...'});
  try{
    const ab = base64ToArrayBuffer(file.data.split(',')[1]);
    const dec = await (new (window.AudioContext || window.webkitAudioContext)()).decodeAudioData(ab);
    const offline = new OfflineAudioContext(dec.numberOfChannels, dec.length, dec.sampleRate);
    const src = offline.createBufferSource(); src.buffer = dec;
    const hp = offline.createBiquadFilter(); hp.type='highpass'; hp.frequency.value = 120;
    const lp = offline.createBiquadFilter(); lp.type='lowpass'; lp.frequency.value = 12000;
    const comp = offline.createDynamicsCompressor(); comp.threshold.value=-40; comp.ratio=6;
    src.connect(hp); hp.connect(lp); lp.connect(comp); comp.connect(offline.destination);
    src.start(0);
    const rendered = await offline.startRendering();
    // normalize
    for(let ch=0; ch<rendered.numberOfChannels; ch++){
      const data = rendered.getChannelData(ch); let peak=0; for(let i=0;i<data.length;i++) peak=Math.max(peak, Math.abs(data[i])); if(peak>0){ const f=0.95/peak; for(let i=0;i<data.length;i++) data[i]*=f; }
    }
    const wav = audioBufferToWav(rendered);
    const blob = new Blob([wav], {type:'audio/wav'});
    const url = URL.createObjectURL(blob);
    sendResult({text:'Enhanced audio ready', file:{name:'enhanced.wav', data: url}});
  }catch(e){ sendResult({text:'Audio enhance failed: '+e.message}); }
}

/* 8) Voice transcribe: placeholder (Whisper requires API), we attempt naive fallback */
async function handleVoiceTranscribe(file){
  sendResult({text:'Transcription placeholder. For real transcribe, save OpenAI/Whisper or use server. (Demo)'}); 
}

/* 9) Text summarizer (naive) */
async function handleSummarize(prompt){
  if(!prompt) return sendResult({text:'Paste text to summarize.'});
  const s = naiveSumm(prompt);
  sendResult({text:'Summary:\n\n'+s});
}
function naiveSumm(s){
  const parts = s.split(/(?<=[.?!])\s+/).filter(Boolean);
  parts.sort((a,b)=>b.length-a.length);
  return parts.slice(0,2).join(' ');
}

/* 10) Text rewrite (naive) */
async function handleRewrite(prompt){
  if(!prompt) return sendResult({text:'Paste text to rewrite.'});
  const out = prompt.split(/(?<=[.?!])\s+/).map(x=> x.length>120 ? x.slice(0,90)+'...' : x).join(' ');
  sendResult({text:'Rewritten:\n\n'+out});
}

/* 11) TTS: return message (parent uses speechSynthesis) */
async function handleTTS(prompt){
  if(!prompt) return sendResult({text:'Provide text for TTS.'});
  // send text back; parent can play via speechSynthesis or embed audio URL after server TTS
  sendResult({text:'TTS ready (use browser SpeechSynthesis):\n\n'+prompt});
}

/* 12) Meme maker (simple overlay) */
async function handleMeme(file, prompt){
  if(!file || !file.data) return sendResult({text:'Upload base image for meme.'});
  const img = await loadImageFromDataURL(file.data);
  const c = document.createElement('canvas'); c.width=img.width; c.height=img.height;
  const cx = c.getContext('2d'); cx.drawImage(img,0,0);
  // prompt -> top|bottom (split by ; )
  const [topText, bottomText] = (prompt || '').split(';').map(s=>s? s.trim() : '');
  cx.fillStyle='white'; cx.strokeStyle='black'; cx.textAlign='center'; cx.font = Math.floor(c.width/12)+'px Impact';
  if(topText){ cx.fillText(topText, c.width/2, Math.floor(c.height*0.12)); cx.strokeText(topText, c.width/2, Math.floor(c.height*0.12)); }
  if(bottomText){ cx.fillText(bottomText, c.width/2, Math.floor(c.height*0.92)); cx.strokeText(bottomText, c.width/2, Math.floor(c.height*0.92)); }
  const url = c.toDataURL('image/png');
  sendResult({text:'Meme created', file:{name:'meme.png', data: url}});
}

/* 13) Code assist (placeholder) */
async function handleCodeAssist(prompt){
  if(!prompt) return sendResult({text:'Ask for code snippet.'});
  const sample = `// Demo code for: ${prompt}\nfunction demo(){ console.log("Replace with real model output via API."); }`;
  sendResult({text: sample});
}

/* 14) Upscale placeholder */
async function handleUpscale(file){
  if(!file || !file.data) return sendResult({text:'Upload image first.'});
  sendResult({text:'Upscale placeholder — use ESRGAN/GFPGAN via HuggingFace or your backend for real upscale.'});
}

/* Placeholder generic */
async function placeholderTool(name){ sendResult({text: name + ' is a placeholder in demo. Add API key in Settings or connect backend for full functionality.'}); }

/* -------------------------
  Helpers: base64 -> ArrayBuffer, audioBuffer->wav
  ------------------------- */
function base64ToArrayBuffer(base64){
  const binary = atob(base64);
  const len = binary.length; const bytes = new Uint8Array(len);
  for(let i=0;i<len;i++) bytes[i] = binary.charCodeAt(i);
  return bytes.buffer;
}
function audioBufferToWav(buffer){
  const numChannels = buffer.numberOfChannels, sampleRate = buffer.sampleRate, bitDepth = 16;
  const bytesPerSample = bitDepth/8, blockAlign = numChannels * bytesPerSample;
  const dataSize = buffer.length * blockAlign;
  const arrayBuffer = new ArrayBuffer(44 + dataSize); const view = new DataView(arrayBuffer);
  function writeString(v,offset,s){ for(let i=0;i<s.length;i++) v.setUint8(offset+i,s.charCodeAt(i)); }
  writeString(view,0,'RIFF'); view.setUint32(4,36 + dataSize,true); writeString(view,8,'WAVE');
  writeString(view,12,'fmt '); view.setUint32(16,16,true); view.setUint16(20,1,true); view.setUint16(22,numChannels,true);
  view.setUint32(24,sampleRate,true); view.setUint32(28,sampleRate * blockAlign,true); view.setUint16(32,blockAlign,true); view.setUint16(34,bitDepth,true);
  writeString(view,36,'data'); view.setUint32(40,dataSize,true);
  let offset = 44;
  const channels = []; for(let i=0;i<numChannels;i++) channels.push(buffer.getChannelData(i));
  for(let i=0;i<buffer.length;i++){
    for(let ch=0;ch<numChannels;ch++){
      let sample = Math.max(-1, Math.min(1, channels[ch][i]));
      view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true);
      offset += 2;
    }
  }
  return arrayBuffer;
}

/* export helper for parent debugging */
window.testSend = (m)=> sendResult({text: 'test: '+m});

/* End of backend script */
log('Backend iframe ready (demo) — waiting for parent messages.');
</script>
</body>
</html>
